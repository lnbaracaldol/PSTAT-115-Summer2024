---
title: "Homework 1"
author: "PSTAT 115, Summer 2024"
date: "__Due on Wednesday, July 3rd, 2024 at 11:59 pm__"
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
library(knitr)
library(testthat)
library(tidyverse)
library(patchwork)
eval <- TRUE
knitr::opts_chunk$set(echo=TRUE, 
                      cache=FALSE, 
                      fig.width=5, 
                      fig.height=5,
                      fig.align='center', 
                      eval=eval)
indent1 = '    '
indent2 = paste(rep(indent1, 2), collapse='')
indent3 = paste(rep(indent1, 3), collapse='')
r = function(x, digits=2){ round(x, digits=digits) }

```

# Text Analysis of JK Rowling's Harry Potter Series

# Question 1
You are interested in studying the writing style and tone used by JK Rowling (JKR for short), the author of the popular Harry Potter series.  You select a random sample of chapters of size $n$ from all of JKR's books.  You are interested in the rate at which JKR uses the word _stone_ in her writing, so you count how many times the word _stone_ appears in each chapter in your sample, $(y_1,...,y_n)$. In this set-up, $y_i$ is the  number of times the word _stone_ appeared in the $i$-th randomly sampled chapter. In this context, the population of interest is all chapters written by JRK and the population quantity of interest (the estimand) is the rate at which JKR uses the word _stone_. The sampling units are individual chapters. Note: this assignment is partially based on text analysis package known as [tidytext](https://www.tidytextmining.com/tidytext.html]).  You can read more about tidytext [here](https://uc-r.github.io/tidy_text).  



## 1a. (5 pts)
Model: let $Y_i$ denote the quantity that captures the number of times the word _stone_ appears in the $i$-th chapter.  As a first approximation, it is reasonable to model the number of times _stone_ appears in a given chapter using a  Poisson distribution. _Reminder:_ Poisson distributions are for integer outcomes and useful for events that occur independently and at a constant rate.  Let's assume that the quantities $Y_1,...Y_n$ are independent and identically distributed (IID) according to a Poisson distribution with unknown parameter $\lambda$,
        $$p(Y_i=y_i\mid\lambda) = \hbox{Poisson} (y_i \mid\lambda) \quad \hbox{ for } \quad     i=1,...,n.$$

Write the likelihood $L(\lambda)$ for a generic sample of $n$ chapters, $(y_1,...,y_n)$. Simplify as much as possible (i.e. get rid of any multiplicative constants)  

## 1b. (5 pts)

Write the log-likelihood $\ell(\lambda)$ for a generic sample of $n$ articles, $(y_1,...,y_n)$.  Simplify as much as possible.  Use this to compute the maximum likelihood estimate for the rate parameter of the Poisson distribution.    

From now on, we'll focus on JKR's writing style in the first Harry Potter book, _Philosopher's Stone_.  This book has 17 chapters. Below is the code for counting the number of times _stone_ appears in each chapter of _Philosopher's Stone_ .  We use the `tidytext` R package which includes functions that parse large text files into word counts.  The code below creates a vector of length 17 which has the number of times the word _stone_ was used in that chapter (see https://uc-r.github.io/tidy_text for more on parsing text with `tidytext`)
    
```{r parse_text}
library(tidyverse)      # data manipulation & plotting
library(stringr)        # text cleaning and regular expressions
library(tidytext)       # provides additional text mining functions
library(harrypotter)    # text for the seven novels of the Harry Potter series

text_tb <- tibble(chapter = seq_along(philosophers_stone),
                  text = philosophers_stone)
tokens <- text_tb %>% unnest_tokens(word, text)
word_counts <- tokens %>% group_by(chapter) %>% 
  count(word, sort = TRUE) %>% ungroup
word_counts_mat <- word_counts %>% spread(key=word, value=n, fill=0)

stone_counts <- word_counts_mat$stone
```

## 1c. (5 pts)

Make a bar plot where the heights are the counts of the word _stone_ and the x-axis is the chapter.  


## 1d. (10 pts)
Plot the log-likelihood of the Poisson rate of _stone_ usage in R using the data in `stone_counts`.  Then use `stone_counts` to compute the maximum likelihood estimate of the rate of the usage of the word _stone_ in Philosopher's Stone.  Mark this maximum on the log-likelihood plot with a vertical line (use `abline` if you make the plot in base R or `geom_vline` if you prefer `ggplot`).  



# Question 2

For the previous problem, when computing the rate of _stone_ usage, we were implicitly assuming each chapter had the same length.  Remember that for $Y_i \sim \text{Poisson}(\lambda)$, $E[Y_i] = \lambda$ for each chapter, that is, the average number of occurrences of _stone_ is the same in each chapter.  Obviously this isn't a great assumption, since the lengths of the chapters vary; longer chapters should be more likely to have more occurrences of the word.  We can augment the model by considering properties of the Poisson distribution.  The Poisson is often used to express the probability of a given number of events occurring for a fixed ``exposure''.  As a useful example of the role of the exposure term, when counting then number of events that happen in a set length of time, we to need to account for the total time that we are observing events.  For this text example, the exposure is not time, but rather corresponds to the total length of the chapter.  

We will again let $(y_1,...,y_n)$ represent counts of the word _stone_. In addition, we now count the total number of words in each each chapter $(\nu_1,...,\nu_n)$ and use this as our exposure.  Let $Y_i$ denote the random variable for the counts of the word _stone_ in a chapter with $\nu_i$ words.  Let's assume that the quantities $Y_1,...Y_n$ are independent and distributed according to a Poisson distribution with unknown parameter $\lambda \cdot \frac{\nu_i}{1000}$,
$$
p(Y_i=y_i\mid \nu_i, 1000) = \hbox{Poisson} (y_i \mid\lambda \cdot \frac{\nu_i}{1000}) \quad \hbox{ for } \quad i=1,...,n.
$$

In the code below, `chapter_lengths` is a vector storing the length of each chapter in words.  

```{r chapter_lengths, dependson="parse_text", out.width="50%"}
chapter_lengths <- word_counts %>% group_by(chapter) %>% 
summarize(chapter_length = sum(n)) %>% 
ungroup %>% select(chapter_length) %>% unlist %>% as.numeric
```


## 2a. (5 pts)

What is the interpretation of the quantity $\frac{\nu_i}{1000}$ in this model?  What is the interpretation of $\lambda$ in this model? State the units for these quantities in both of your answers.


## 2b. (5 pts)

List the known and unknown variables and constants, as described in lectures 1 and 2.  Make sure you include $Y_1, ..., Y_n$, $y_1, ..., y_n$, $n$, $\lambda$, and $\nu_i$.



## 2c. (5 pts)

Write down the likelihood in this new model.  Use this to calculate maximum likelihood estimator for $\lambda$.  Your answer should include the $\nu_i$'s.  


## 2d. (5 pts)

Compute the maximum likelihood estimate and save it in the variable `lambda_mle`. In 1-2 sentences interpret its meaning (make sure you include units in your answers!).  


## 2e. (10 pts)

Plot the log-likelihood from the previous question in R using the data from on the frequency of _stone_ and the chapter lengths.  Add a vertical line at the value of `lambda_mle` to indicate the maximum likelihood.



# Question 3
Correcting for chapter lengths is clearly an improvement, but we're still assuming that JKR uses the word _stone_ at the same rate in all chapters. In this problem we'll explore this assumption in more detail.


## 3a. (5 pts)
Why might it be unreasonable to assume that the rate of _stone_ usage is the same in all chapters? Comment in a few sentences.


## 3b. (2 + 4 pts)
We can use simulation to check our Poisson model, and in particular the assumption that the rate of _stone_ usage is the same in all chapters.  Generate simulated counts of the word _stone_ by sampling counts from a Poisson distribution with the rate $(\hat \lambda_{\text{MLE}} \nu_i) /1000$ for each chapter $i$. $\hat \lambda_{\text{MLE}}$ is the maximum likelihood estimate computing in 2d.  Store the vector of these values for each chapter in a variable of length 17 called `lambda_chapter`.  Make a side by side plot of the observed counts and simulated counts and note any similarities or differences (we've already created the observed histogram for you).  Are there any outliers in the observed data that don't seem to be reflected in the data simulated under our model?  

```{r}
# observed_histogram <- ggplot(word_counts_mat) + geom_histogram(aes(x=stone)) + 
  # xlim(c(0, 25)) + ylim(c(0,10)) + ggtitle("Observed")

#lambda_chapter <- ... # SOLUTION

# simulated_counts <- tibble(stone = rpois(17, lambda_chapter))

#simulated_histogram <- ggplot(simulated_counts) + geom_histogram(aes(x=stone)) + ...

## This uses the patchwork library to put the two plots side by side
# observed_histogram + simulated_histogram

```

## 3c. (5 + 5 pts)

Assume the word usage rate varies by chapter, that is, 
$$
p(Yi=y_i\mid \lambda, \nu_i, 1000) = \hbox{Poisson} (y_i \mid\lambda_i \cdot \frac{\nu_i}{1000}) \quad \hbox{ for } \quad i=1,...,n.
$$


Compute a separate maximum likelihood estimate of the rate of _stone_ usage (per 1000 words) in each chapter, $\hat \lambda_i$.  Make a bar plot of $\hat \lambda_i$ by chapter. Save the chapter-specific MLE in a vector of length 17 called `lambda_hats`.  Which chapter has the highest rate of usage of the word _stone_? Save the chapter number in a variable called `stonest_chapter`.

```{r, dependson="chapter_lengths"}

# Maximum likelihood estimate
# lambda_hats <- ... # SOLUTION

# stonest_chapter <- ... # SOLUTION

# Make a bar plot of the MLEs, lambda_hats
```
    


# Question 4

Let's go back to our original model for usage rates of the word _stone_.  You collect a random sample of book chapters penned by JKR and count how many times she uses the word _stone_ in each of the chapter in your sample, $(y_1,...,y_n)$.  In this set-up, $y_i$ is the  number of times the word _stone_ appeared in the $i$-th chapter, as before.  However, we will no longer assume that the rate of use of the word _stone_ is the same in every chapter.  Rather, we'll assume JKR uses the word _stone_ at different rates $\lambda_i$ in each chapter.   Naturally, this makes sense, since different chapters have different themes.  To do this, we'll further assume that the rate of word usage $\lambda_i$ itself, is distributed according to a Gamma($\alpha$, $\beta$) with known parameters $\alpha$ and $\beta$,
  $$
   f(\Lambda=\lambda_i\mid \alpha,\beta) = \hbox{Gamma} (\lambda_i \mid\alpha,\beta).
  $$
and that $Y_i \sim \text{Pois}(\lambda_i)$ as in problem 1.  For now we will ignore any exposure parameters, $\nu_i$.  Note: this is a "warm up" to Bayesian inference, where it is standard to treat parameters as random variables and specify distributions for those parameters.  

## 4a. (5 pts)

Write out the the data generating process for the above model. 


## 4b. (2 + 4 + 4 pts)

In R simulate 1000 values from the above data generating process, assume $\alpha=10$ (shape parameter of `rgamma`) and $\beta=1$ (rate parameter of `rgamma`).  Store the value in a vector of length 1000 called `counts`.  Compute the empirical mean and variance of values you generated.  For a Poisson distribution, the mean and the variance are the same.  In the following distribution is the variance greater than the mean (called ``overdispsersed'') or is the variance less than the mean (``underdispersed'')? Intuitively, why does this make sense?

```{r}
## Store simulated data in a vector of length 1000

# ...

# print(mean(counts))
# print(var(counts))

```
    

## 4c. (5 pts)

List the known and unknown variables and constants as described in lecture 2.  Make sure your table includes $Y_1, ..., Y_n$, $y_1, ..., y_n$, $n$, $\lambda$, $\alpha$, and $\beta$.  



# Question 5 (4 pts)
Compute $p(Y_i \mid \alpha, \beta) = \int p(Y_i, \lambda_i \mid \alpha, \beta) d\lambda_i$.  _Hint:_ The gamma function is defined as $\Gamma ( z ) = \int _ { 0 } ^ { \infty } x ^ { z - 1 } e ^ { - x } d x$. _NEW HINT:_ From the Gamma distribution, we can see that $\frac{\Gamma(\alpha)}{\beta^{\alpha}} = \int_0^{\infty} x^{\alpha-1}e^{-\beta x}dx$.

Fill in the blank.

You just showed that a Gamma mixture of Poisson distributions is a _______.  
