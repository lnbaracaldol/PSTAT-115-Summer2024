---
title: "Homework 3"
author: "PSTAT 115, summer 2024"
date: "__Due on July 26th, 2024 at 11:59 pm__"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
library(testthat)
eval <- TRUE
knitr::opts_chunk$set(eval=eval,
                      echo=TRUE, 
                      cache=FALSE, 
                      fig.width=5, 
                      fig.height=5,
                      fig.align='center')
indent1 = '    '
indent2 = paste(rep(indent1, 2), collapse='')
indent3 = paste(rep(indent1, 3), collapse='')
r = function(x, digits=2){ round(x, digits=digits) }
library(tidyverse)
library(reshape2)
```


**1. Posterior Credible Intervals**  

One way for us to learn more about posterior distributions is to find some credible intervals. Suppose we have a posterior density of a parameter $\lambda$, defined by $\lambda|y \sim \text{Gamma}(4, 1)$.

a. (7pts) Plot this distribution. Construct the posterior middle 95% credible interval for $\lambda$. The middle 95% interval is such that 2.5% of the probability mass is left to the left and 2.5% to the right.  Save the lower and upper endpoints in a vector of length 2, called `middle_95`.  Using either line segments or shading, display this interval on the plot.

b. (5pts) Interpret the interval you found. How is it different from a frequentist confidence interval?

c. (8pts) Besides the middle 95% credible interval, we could also find the 95% highest posterior density (HPD) region. This region  contains the 95% of posterior values with the highest posterior densities. The HPD region will always be the shortest credible interval for a given probability, since it by definition contains the values of $\lambda$ with the highest probability of occurring.
Use `HDInterval::hdi()` to construct the HPD region. Save the lower and upper endpoints of this region in a variable called `hdi_region`. Add this interval to the plot you made in part (a), making sure that both intervals are distinguishable on the plot.

d. (5pts) Based on your plot, how do the two kinds of 95% credible intervals differ? How long is the middle interval? The HDI interval?




 
**2. Cancer Research in Laboratory Mice**

A laboratory is estimating the rate of tumorigenesis (the formation of tumors) in two strains of mice, A and B.  They have tumor count data for 10 mice in strain A and 13 mice in strain B.  Type A mice have been well studied, and information from other laboratories suggests that type A mice have tumor counts that are approximately Poisson-distributed. Tumor count rates for type B mice are unknown, but type B mice are related to type A mice. Assuming a Poisson sampling distribution for each group with rates $\theta_A$ and $\theta_B$. We assume $\theta_A \sim \text{gamma}(120, 10)$ and  $\theta_B\sim\text{gamma}(12, 1)$.  We observe $y_A = (12,9,12,14,13,13,15,8,15,6)$ and
$y_B = (11,11,10,9,9,8,7,10,6,8,8,9,7)$. Now we will actually investigate evidence that Type A mice have higher rates of tumor formation than Type B mice.  

a. Obtain $Pr(\theta_B < \theta_A \mid y_A, y_B)$ via Monte Carlo sampling. Report the value.

```{r}
y_A <- c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)
y_B <- c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)

# store your probabilities in a vector called "pr" for testing.

# YOUR CODE HERE
pr <- NULL # YOUR CODE HERE
print(pr)

```

b. Now compute $P\tilde Y_B < \tilde Y_A \mid Y_B, Y_A)$, where $\tilde Y_A$ and $\tilde Y_B$ are samples from the posterior predictive distribution. 

```{r}
y_A <- c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)

# store your probabilities in a vector called "ppr" for testing.

# YOUR CODE HERE
ppr <- NULL # YOUR CODE HERE
print(ppr)
```

c.  In the context of this problem, describe the meaning of the events $\{\theta_B < \theta_A\}$ and $\{\tilde Y_B < \tilde Y_A\}$.  How are they different? Why do the relative values of the answers in parts a and b make sense?

_Type your answer here, replacing this text._

**3. Posterior Predictive Model Checking**

Model checking and refinement is an essential part of Bayesian data analysis. Let's investigate the adequacy of the Poisson model for the tumor count data. Consider strain A mice only for now, and generate posterior predictive datasets $y_B^{(1)}, ..., y_A^{(1000)}$. Each $y_B^{(s)}$ is a sample of size $n_B = 13$ from the Poisson distribution with parameter $\theta_B^{(s)}$, $\theta_B^{(s)}$ is itself a sample from the posterior distribution $p(\theta_B \mid y_B)$ and $y_B$ is the observed data.  For each $s$, let $t^{(s)}$ be the sample average divided by the sample variance of $y_B^{(s)}$.

a.  If the Poisson model was a reasonable one, what would a "typical" value $t^{(s)}$ be? Why?

_Type your answer here, replacing this text._

b.  In any given experiment, the realized value of $t^{s}$ will not be exactly the "typical value" due to sampling variability.  Make a histogram of $t^{(s)}$ and compare to the observed value of this statistic, $\frac{\text{mean}(y_A)}{\text{var}(y_B)}$. Can sampling variability alone explain the observed test statistic? It may help to compute the fraction of posterior predictive draws which are larger than the observed draws. Make a comment on if the Poisson model seems reasonable for these data (at least by this one metric).  

```{r}
y_B = c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)

# generate posterior predictive datasets and find test statistic for each one
# store your test statistics in a vector called "tb" for testing

# YOUR CODE HERE
```



```{r}
# create the histogram, adding a vertical line at the observed value of the test statistic
# YOUR CODE HERE
```

_Type your answer here, replacing this text._

c. When the mean is less than the variance we say that the data is _overdispersed_.  When the mean is more than the variance we say that the data is _underdispersed_.  Do you have any evidence that the data is underdispered? Overdispered?